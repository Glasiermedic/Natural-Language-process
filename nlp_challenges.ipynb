{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_challenges.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Glasiermedic/Natural-Language-process/blob/master/nlp_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDST-Lu0T6J4",
        "colab_type": "code",
        "outputId": "25cd3b72-1196-44dd-fdd3-c3726e06957b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "import math\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import ensemble\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Display preferences.\n",
        "%matplotlib inline\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "# Suppress annoying harmless error.\n",
        "warnings.filterwarnings(\n",
        "    action=\"ignore\",\n",
        "    module=\"scipy\",\n",
        "    message=\"^internal gelsd\"\n",
        ")\n",
        "\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import neighbors\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, Imputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import os\n",
        "\n",
        "import pydotplus\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n",
        "sns.set_style('white')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PHfehA004YZ",
        "colab_type": "code",
        "outputId": "d65f1e14-9231-4241-b9dc-9c2a1d3ec47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79kAuoob__Rd",
        "colab_type": "code",
        "outputId": "7caaeca7-30a2-498a-f9ac-f14ae8a5d761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxA1KRI34Gmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from nltk.corpus import gutenberg, stopwords\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdV6HPFI6FX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print (gutenberg.fileids())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsm8VaGf_Gxm",
        "colab_type": "text"
      },
      "source": [
        "Time to bag some words! Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur. We will exclude stopwords and punctuation. In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_5MADijnU9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function for standard text cleaning.\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "    \n",
        "# Load and clean the data.\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# The Chapter indicator is idiosyncratic\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "    \n",
        "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
        "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqfE_c-XnouD",
        "colab_type": "code",
        "outputId": "d7bd7a69-2d4a-4047-f058-6e8163a63abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Parse the cleaned novels. This can take a bit.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.12 s, sys: 216 ms, total: 2.34 s\n",
            "Wall time: 2.34 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_pGGwbynrUv",
        "colab_type": "code",
        "outputId": "de115588-2af3-4bbb-a8bf-4b611388b417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Group into sentences.\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# Combine the sentences from the two novels into one data frame.\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
        "sentences.head(20)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(A, WATCH, OUT, OF, ITS, WAISTCOAT, -, POCKET,...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(,, Alice, started, to, her, feet, ,, for, it,...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(In, another, moment, down, went, Alice, after...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(The, rabbit, -, hole, went, straight, on, lik...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(Either, the, well, was, very, deep, ,, or, sh...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(First, ,, she, tried, to, look, down, and, ma...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(;, then, she, looked, at, the, sides, of, the...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(She, took, down, a, jar, from, one, of, the, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(Well, !, ', thought, Alice, to, herself, ,, '...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(How, brave, they, 'll, all, think, me, at, ho...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(Why, ,, I, would, n't, say, anything, about, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>((, Which, was, very, likely, true, ., ))</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(Down, ,, down, ,, down, .)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0        1\n",
              "0   (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1   (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2   (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                       (Oh, dear, !)  Carroll\n",
              "4                                       (Oh, dear, !)  Carroll\n",
              "5                          (I, shall, be, late, !, ')  Carroll\n",
              "6   ((, when, she, thought, it, over, afterwards, ...  Carroll\n",
              "7   (A, WATCH, OUT, OF, ITS, WAISTCOAT, -, POCKET,...  Carroll\n",
              "8   (,, Alice, started, to, her, feet, ,, for, it,...  Carroll\n",
              "9   (In, another, moment, down, went, Alice, after...  Carroll\n",
              "10  (The, rabbit, -, hole, went, straight, on, lik...  Carroll\n",
              "11  (Either, the, well, was, very, deep, ,, or, sh...  Carroll\n",
              "12  (First, ,, she, tried, to, look, down, and, ma...  Carroll\n",
              "13  (;, then, she, looked, at, the, sides, of, the...  Carroll\n",
              "14  (She, took, down, a, jar, from, one, of, the, ...  Carroll\n",
              "15  (Well, !, ', thought, Alice, to, herself, ,, '...  Carroll\n",
              "16  (How, brave, they, 'll, all, think, me, at, ho...  Carroll\n",
              "17  (Why, ,, I, would, n't, say, anything, about, ...  Carroll\n",
              "18          ((, Which, was, very, likely, true, ., ))  Carroll\n",
              "19                        (Down, ,, down, ,, down, .)  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W34vvu5_uti",
        "colab_type": "text"
      },
      "source": [
        "Time to bag some words! Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur. We will exclude stopwords and punctuation. In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBqglT7nn6FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to create a list of the 2000 most common words.\n",
        "def bag_of_words(text):\n",
        "    \n",
        "    # Filter out punctuation and stop words.\n",
        "    allwords = [token.lemma_\n",
        "                for token in text\n",
        "                if not token.is_punct\n",
        "                and not token.is_stop]\n",
        "    \n",
        "    # Return the most common words.\n",
        "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
        "    \n",
        "\n",
        "# Creates a data frame with features for each word in our common word set.\n",
        "# Each value is the count of the times the word appears in each sentence.\n",
        "def bow_features(sentences, common_words):\n",
        "    \n",
        "    # Scaffold the data frame and initialize counts to zero.\n",
        "    df = pd.DataFrame(columns=common_words)\n",
        "    df['text_sentence'] = sentences[0]\n",
        "    df['text_source'] = sentences[1]\n",
        "    df.loc[:, common_words] = 0\n",
        "    \n",
        "    # Process each row, counting the occurrence of words in each sentence.\n",
        "    for i, sentence in enumerate(df['text_sentence']):\n",
        "        \n",
        "        # Convert the sentence to lemmas, then filter out punctuation,\n",
        "        # stop words, and uncommon words.\n",
        "        words = [token.lemma_\n",
        "                 for token in sentence\n",
        "                 if (\n",
        "                     not token.is_punct\n",
        "                     and not token.is_stop\n",
        "                     and token.lemma_ in common_words\n",
        "                 )]\n",
        "        \n",
        "        # Populate the row with word counts.\n",
        "        for word in words:\n",
        "            df.loc[i, word] += 1\n",
        "        \n",
        "        # This counter is just to make sure the kernel didn't hang.\n",
        "        if i % 50 == 0:\n",
        "            print(\"Processing row {}\".format(i))\n",
        "            \n",
        "    return df\n",
        "\n",
        "# Set up the bags.\n",
        "alicewords = bag_of_words(alice_doc)\n",
        "persuasionwords = bag_of_words(persuasion_doc)\n",
        "\n",
        "# Combine bags to create a set of unique words.\n",
        "common_words = set(alicewords + persuasionwords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79iMJnysn7e6",
        "colab_type": "code",
        "outputId": "22e32453-961b-4932-a15b-7c5c2925e67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(sentences, common_words)\n",
        "word_counts.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n",
            "Processing row 400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>market</th>\n",
              "      <th>indifference</th>\n",
              "      <th>little</th>\n",
              "      <th>unite</th>\n",
              "      <th>public</th>\n",
              "      <th>line</th>\n",
              "      <th>listen</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>St</th>\n",
              "      <th>ball</th>\n",
              "      <th>decide</th>\n",
              "      <th>milk</th>\n",
              "      <th>furniture</th>\n",
              "      <th>seen</th>\n",
              "      <th>go</th>\n",
              "      <th>acquaint</th>\n",
              "      <th>confirm</th>\n",
              "      <th>charity</th>\n",
              "      <th>faculty</th>\n",
              "      <th>volume</th>\n",
              "      <th>comprehensive</th>\n",
              "      <th>night</th>\n",
              "      <th>child</th>\n",
              "      <th>history</th>\n",
              "      <th>agreeable</th>\n",
              "      <th>...</th>\n",
              "      <th>Trafalgar</th>\n",
              "      <th>matter</th>\n",
              "      <th>secret</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>ah</th>\n",
              "      <th>hair</th>\n",
              "      <th>world</th>\n",
              "      <th>aggravation</th>\n",
              "      <th>house</th>\n",
              "      <th>sorry</th>\n",
              "      <th>offensive</th>\n",
              "      <th>application</th>\n",
              "      <th>picture</th>\n",
              "      <th>exceed</th>\n",
              "      <th>near</th>\n",
              "      <th>grandson</th>\n",
              "      <th>acre</th>\n",
              "      <th>attraction</th>\n",
              "      <th>power</th>\n",
              "      <th>true</th>\n",
              "      <th>liberality</th>\n",
              "      <th>charm</th>\n",
              "      <th>lavish</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1614 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  market indifference little unite public line listen accidentally St ball  \\\n",
              "0      0            0      0     0      0    0      0            0  0    0   \n",
              "1      0            0      0     0      0    0      0            0  0    0   \n",
              "2      0            0      0     0      0    0      0            0  0    0   \n",
              "3      0            0      0     0      0    0      0            0  0    0   \n",
              "4      0            0      0     0      0    0      0            0  0    0   \n",
              "\n",
              "  decide milk furniture seen go acquaint confirm charity faculty volume  \\\n",
              "0      0    0         0    0  0        0       0       0       0      0   \n",
              "1      0    0         0    0  0        0       0       0       0      0   \n",
              "2      0    0         0    0  0        0       0       0       0      0   \n",
              "3      0    0         0    0  0        0       0       0       0      0   \n",
              "4      0    0         0    0  0        0       0       0       0      0   \n",
              "\n",
              "  comprehensive night child history agreeable  ... Trafalgar matter secret  \\\n",
              "0             0     0     0       0         0  ...         0      0      0   \n",
              "1             0     0     0       0         0  ...         0      0      0   \n",
              "2             0     0     0       0         0  ...         0      0      0   \n",
              "3             0     0     0       0         0  ...         0      0      0   \n",
              "4             0     0     0       0         0  ...         0      0      0   \n",
              "\n",
              "  acceptable ah hair world aggravation house sorry offensive application  \\\n",
              "0          0  0    0     0           0     0     0         0           0   \n",
              "1          0  0    0     0           0     0     0         0           0   \n",
              "2          0  0    0     0           0     0     0         0           0   \n",
              "3          0  0    0     0           0     0     0         0           0   \n",
              "4          0  0    0     0           0     0     0         0           0   \n",
              "\n",
              "  picture exceed near grandson acre attraction power true liberality charm  \\\n",
              "0       2      0    0        0    0          0     0    0          0     0   \n",
              "1       0      0    0        0    0          0     0    0          0     0   \n",
              "2       0      0    0        0    0          0     0    0          0     0   \n",
              "3       0      0    0        0    0          0     0    0          0     0   \n",
              "4       0      0    0        0    0          0     0    0          0     0   \n",
              "\n",
              "  lavish                                      text_sentence text_source  \n",
              "0      0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
              "1      0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
              "2      0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
              "3      0                                      (Oh, dear, !)     Carroll  \n",
              "4      0                                      (Oh, dear, !)     Carroll  \n",
              "\n",
              "[5 rows x 1614 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8odulBiZAHIU",
        "colab_type": "text"
      },
      "source": [
        "###Trying out BoW\n",
        "####Now let's give the bag of words features a whirl by trying a random forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mBlfDSoGsg",
        "colab_type": "code",
        "outputId": "b3f6c68f-b5ca-4e5d-e356-dee8fcb79bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "rfc = ensemble.RandomForestClassifier()\n",
        "Y = word_counts['text_source']\n",
        "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y,\n",
        "                                                    test_size=0.4,\n",
        "                                                    random_state=1)\n",
        "train = rfc.fit(X_train, y_train)\n",
        "\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9736842105263158\n",
            "\n",
            "Test set score: 0.8314606741573034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo2HmN8XoAKX",
        "colab_type": "code",
        "outputId": "80425862-c741-408c-db07-7a0f0d028e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(penalty='l2', solver = 'lbfgs', max_iter = 1000, C = 100) # No need to specify l2 as it's the default. But we put it for demonstration.\n",
        "train = lr.fit(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(266, 1612) (266,)\n",
            "Training set score: 0.9849624060150376\n",
            "\n",
            "Test set score: 0.8707865168539326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdCQy3aooSnW",
        "colab_type": "code",
        "outputId": "386d8e7d-ab45-45f8-ced7-52e8fc4c8651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "clf = ensemble.GradientBoostingClassifier()\n",
        "train = clf.fit(X_train, y_train)\n",
        "\n",
        "print('Training set score:', clf.score(X_train, y_train))\n",
        "print('\\nTest set score:', clf.score(X_test, y_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9774436090225563\n",
            "\n",
            "Test set score: 0.8089887640449438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFOfgIHKoSfa",
        "colab_type": "code",
        "outputId": "d556f58d-d278-4c21-87e2-564b9ff77aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Clean the Emma data.\n",
        "emma = gutenberg.raw('austen-emma.txt')\n",
        "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
        "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
        "emma = text_cleaner(emma[:int(len(emma)/60)])\n",
        "print(emma[:100])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so1iLglUoa3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse our cleaned data.\n",
        "emma_doc = nlp(emma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIn7yYIuob0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Group into sentences.\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTl99xf7ofIn",
        "colab_type": "code",
        "outputId": "41a19341-7bb4-479d-e8e3-80024825ebe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Build a new Bag of Words data frame for Emma word counts.\n",
        "# We'll use the same common words from Alice and Persuasion.\n",
        "emma_sentences = pd.DataFrame(emma_sents)\n",
        "emma_bow = bow_features(emma_sentences, common_words)\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxgXxivoohjZ",
        "colab_type": "code",
        "outputId": "73bf9046-5e53-45ad-a5ed-af4a66058790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "# Now we can model it!\n",
        "# Let's use logistic regression again.\n",
        "\n",
        "# Combine the Emma sentence data with the Alice data from the test set.\n",
        "X_Emma_test = np.concatenate((\n",
        "    X_train[y_train[y_train=='Carroll'].index],\n",
        "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
        "), axis=0)\n",
        "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
        "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
        "\n",
        "# Model.\n",
        "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
        "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
        "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set score: 0.6341463414634146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>Austen</th>\n",
              "      <th>Carroll</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Austen</th>\n",
              "      <td>136</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Carroll</th>\n",
              "      <td>56</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    Austen  Carroll\n",
              "row_0                   \n",
              "Austen      136       34\n",
              "Carroll      56       20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTYQ4lLvopPC",
        "colab_type": "text"
      },
      "source": [
        "### Challenge 0:\n",
        "#### Recall that the logistic regression model's best performance on the test set was 93%. See what you can do to improve performance. Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires. Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DbHGd1-olLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "kfolds = StratifiedShuffleSplit(n_splits =10, random_state=1)\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8KIh63k2ira",
        "colab_type": "code",
        "outputId": "48670132-95f9-4209-8406-074be172a4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "word_counts.info()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 444 entries, 0 to 443\n",
            "Columns: 1614 entries, market to text_source\n",
            "dtypes: object(1614)\n",
            "memory usage: 5.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvii1Eov5ujN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counts2 = word_counts.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UCPE9NH6bOV",
        "colab_type": "code",
        "outputId": "7fb5a28e-a1e6-4639-9975-9c3b5f73e8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "word_counts2.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>market</th>\n",
              "      <th>indifference</th>\n",
              "      <th>little</th>\n",
              "      <th>unite</th>\n",
              "      <th>public</th>\n",
              "      <th>line</th>\n",
              "      <th>listen</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>St</th>\n",
              "      <th>ball</th>\n",
              "      <th>decide</th>\n",
              "      <th>milk</th>\n",
              "      <th>furniture</th>\n",
              "      <th>seen</th>\n",
              "      <th>go</th>\n",
              "      <th>acquaint</th>\n",
              "      <th>confirm</th>\n",
              "      <th>charity</th>\n",
              "      <th>faculty</th>\n",
              "      <th>volume</th>\n",
              "      <th>comprehensive</th>\n",
              "      <th>night</th>\n",
              "      <th>child</th>\n",
              "      <th>history</th>\n",
              "      <th>agreeable</th>\n",
              "      <th>...</th>\n",
              "      <th>Trafalgar</th>\n",
              "      <th>matter</th>\n",
              "      <th>secret</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>ah</th>\n",
              "      <th>hair</th>\n",
              "      <th>world</th>\n",
              "      <th>aggravation</th>\n",
              "      <th>house</th>\n",
              "      <th>sorry</th>\n",
              "      <th>offensive</th>\n",
              "      <th>application</th>\n",
              "      <th>picture</th>\n",
              "      <th>exceed</th>\n",
              "      <th>near</th>\n",
              "      <th>grandson</th>\n",
              "      <th>acre</th>\n",
              "      <th>attraction</th>\n",
              "      <th>power</th>\n",
              "      <th>true</th>\n",
              "      <th>liberality</th>\n",
              "      <th>charm</th>\n",
              "      <th>lavish</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1614 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  market indifference little unite public line listen accidentally St ball  \\\n",
              "0      0            0      0     0      0    0      0            0  0    0   \n",
              "1      0            0      0     0      0    0      0            0  0    0   \n",
              "2      0            0      0     0      0    0      0            0  0    0   \n",
              "3      0            0      0     0      0    0      0            0  0    0   \n",
              "4      0            0      0     0      0    0      0            0  0    0   \n",
              "\n",
              "  decide milk furniture seen go acquaint confirm charity faculty volume  \\\n",
              "0      0    0         0    0  0        0       0       0       0      0   \n",
              "1      0    0         0    0  0        0       0       0       0      0   \n",
              "2      0    0         0    0  0        0       0       0       0      0   \n",
              "3      0    0         0    0  0        0       0       0       0      0   \n",
              "4      0    0         0    0  0        0       0       0       0      0   \n",
              "\n",
              "  comprehensive night child history agreeable  ... Trafalgar matter secret  \\\n",
              "0             0     0     0       0         0  ...         0      0      0   \n",
              "1             0     0     0       0         0  ...         0      0      0   \n",
              "2             0     0     0       0         0  ...         0      0      0   \n",
              "3             0     0     0       0         0  ...         0      0      0   \n",
              "4             0     0     0       0         0  ...         0      0      0   \n",
              "\n",
              "  acceptable ah hair world aggravation house sorry offensive application  \\\n",
              "0          0  0    0     0           0     0     0         0           0   \n",
              "1          0  0    0     0           0     0     0         0           0   \n",
              "2          0  0    0     0           0     0     0         0           0   \n",
              "3          0  0    0     0           0     0     0         0           0   \n",
              "4          0  0    0     0           0     0     0         0           0   \n",
              "\n",
              "  picture exceed near grandson acre attraction power true liberality charm  \\\n",
              "0       2      0    0        0    0          0     0    0          0     0   \n",
              "1       0      0    0        0    0          0     0    0          0     0   \n",
              "2       0      0    0        0    0          0     0    0          0     0   \n",
              "3       0      0    0        0    0          0     0    0          0     0   \n",
              "4       0      0    0        0    0          0     0    0          0     0   \n",
              "\n",
              "  lavish                                      text_sentence text_source  \n",
              "0      0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
              "1      0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
              "2      0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
              "3      0                                      (Oh, dear, !)     Carroll  \n",
              "4      0                                      (Oh, dear, !)     Carroll  \n",
              "\n",
              "[5 rows x 1614 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0CdVuiVTh3",
        "colab_type": "code",
        "outputId": "b7b8e28b-fb77-4612-a45a-7d9b29ac227e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "word_counts2['sents_len'] = 0\n",
        "word_counts2['punct_ratio'] = 0\n",
        "word_counts2['stop_ratio'] = 0\n",
        "word_counts2['word_len'] = 0\n",
        "\n",
        "for i, sentence in enumerate(word_counts2['text_sentence']):\n",
        "    punct_count, stop_count = 0, 0\n",
        "    for word in word_counts2.loc[i, 'text_sentence']:\n",
        "        #If a column doesn't exist for the pos_, create it.\n",
        "        if word_counts2.columns.isin([word.pos_]).sum()==0:\n",
        "            word_counts2[word.pos_] = 0\n",
        "        word_counts2.loc[i, word.pos_] += 1\n",
        "        punct_count += word.is_punct\n",
        "        stop_count += word.is_stop\n",
        "    word_counts2.loc[i, 'sents_len'] = len(sentence)\n",
        "    word_counts2.loc[i, 'punct_ratio'] = punct_count/len(sentence)\n",
        "    word_counts2.loc[i, 'stop_ratio'] = stop_count/len(sentence)\n",
        "    word_counts2.loc[i, 'word_len'] = len(str(sentence))/len(sentence)\n",
        "    word_counts2.iloc[i, 3068:] /= len(sentence)\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "        print(i)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwOZyDIF2jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = word_counts2['text_source']\n",
        "X = np.array(word_counts2.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y,\n",
        "                                                    test_size=0.4,\n",
        "                                                    random_state=0)\n",
        "train = rfc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMnfi2O9NObI",
        "colab_type": "code",
        "outputId": "85461525-a76a-4be3-81f9-85d9a813ed9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "lr_params = {'C': [0.0001, 0.01, 1, 100, 10000]}\n",
        "lr_grid = GridSearchCV(lr, lr_params, cv=kfolds)\n",
        "lr_grid.fit(X_train, y_train)\n",
        "print(lr_grid.best_params_)\n",
        "print(lr_grid.best_score_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 100}\n",
            "0.8592592592592593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdwg06PzpeXS",
        "colab_type": "code",
        "outputId": "638ede79-5732-498f-a71c-82185f2038ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(penalty='l2', solver = 'lbfgs', max_iter = 1000, C = 100) # No need to specify l2 as it's the default. But we put it for demonstration.\n",
        "train = lr.fit(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "print(cross_val_score(lr, X_train, y_train, cv=kfolds).mean())  \n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(266, 1630) (266,)\n",
            "Training set score: 0.9924812030075187\n",
            "\n",
            "Test set score: 0.898876404494382\n",
            "0.8592592592592592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbEO_CByJn5n",
        "colab_type": "code",
        "outputId": "791a20d7-a877-4a6a-d7f2-e80303b6f6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(110+31)/(110+22+15+31)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7921348314606742"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bU7FvHoqQrC",
        "colab_type": "text"
      },
      "source": [
        "### Challenge 1:\n",
        "#### Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work. This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L030zalFo7P-",
        "outputId": "9b7946f5-383f-45b2-84a4-8a200b08ec51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Clean the moby_dick data.\n",
        "moby = gutenberg.raw('melville-moby_dick.txt')\n",
        "moby = re.sub(r'VOLUME \\w+', '', moby)\n",
        "moby = re.sub(r'CHAPTER \\w+', '', moby)\n",
        "moby = text_cleaner(moby[:int(len(moby)/60)])\n",
        "print(moby[:100])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ETYMOLOGY. (Supplied by a Late Consumptive Usher to a Grammar School) The pale Usher threadbare in c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CG3jKs7Fo7QC",
        "colab": {}
      },
      "source": [
        "# Parse our cleaned data.\n",
        "moby_doc = nlp(moby)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5Ym6HiFo7QF",
        "colab": {}
      },
      "source": [
        "# Group into sentences.\n",
        "moby_sents = [[sent, 'melville'] for sent in moby_doc.sents]\n",
        "moby_sents = moby_sents[:len(alice_sents)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4h9pTnJho7QH",
        "outputId": "9ace9b6c-e526-4411-f81b-ce38976ba722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "moby_ali_sentences = pd.DataFrame(alice_sents + moby_sents)\n",
        "moby_per_sentences = pd.DataFrame(moby_sents + persuasion_sents)\n",
        "\n",
        "#We'll keep the same common word features so our model will run.\n",
        "moby_ali_bow = bow_features(moby_ali_sentences, common_words)\n",
        "moby_per_bow = bow_features(moby_per_sentences, common_words)\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n",
            "Processing row 400\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD6VlJjmt82V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_features(df):\n",
        "\n",
        "    df['sents_len'] = 0\n",
        "    df['punct_ratio'] = 0\n",
        "    df['stop_ratio'] = 0\n",
        "    df['word_len'] = 0\n",
        "\n",
        "    for i, sentence in enumerate(df['text_sentence']):\n",
        "        punct_count, stop_count = 0, 0\n",
        "        for word in df.loc[i, 'text_sentence']:\n",
        "            #If a column doesn't exist for the pos_, create it.\n",
        "            if df.columns.isin([word.pos_]).sum()==0:\n",
        "                df[word.pos_] = 0\n",
        "            df.loc[i, word.pos_] += 1\n",
        "            punct_count += word.is_punct\n",
        "            stop_count += word.is_stop\n",
        "        df.loc[i, 'sents_len'] = len(sentence)\n",
        "        df.loc[i, 'punct_ratio'] = punct_count/len(sentence)\n",
        "        df.loc[i, 'stop_ratio'] = stop_count/len(sentence)\n",
        "        df.loc[i, 'word_len'] = len(str(sentence))/len(sentence)\n",
        "        df.iloc[i, 3068:] /= len(sentence)\n",
        "    \n",
        "        if i % 500 == 0:\n",
        "            print(i)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzD9BYLPt8vB",
        "colab_type": "code",
        "outputId": "bacec6e8-c6df-4475-fedb-fc84a724b168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_moby_ali = add_features(moby_ali_bow)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYshTD7ewp6Q",
        "colab_type": "code",
        "outputId": "33f37903-4560-48b9-b3e5-3507be6c555d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_moby_per = add_features(moby_per_bow)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NcmLpxB_o7QK",
        "outputId": "6118fd05-3a2d-400b-d596-12da0bbd104b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Now we can model it!\n",
        "# Let's use logistic regression again.\n",
        "df = df_moby_ali.copy()\n",
        "\n",
        "Y = df['text_source']\n",
        "X = np.array(df.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y,\n",
        "                                                    test_size=0.4,\n",
        "                                                    random_state=0)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(penalty='l2', solver = 'lbfgs', max_iter = 1000, C = 100) # No need to specify l2 as it's the default. But we put it for demonstration.\n",
        "train = lr.fit(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "print(cross_val_score(lr, X_train, y_train, cv=kfolds).mean())  \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(154, 1631) (154,)\n",
            "Training set score: 0.9935064935064936\n",
            "\n",
            "Test set score: 0.8076923076923077\n",
            "0.775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IdQpjLBywGp",
        "colab_type": "code",
        "outputId": "ab1622ca-d60b-4f10-aa2c-057645bb1bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "# Now we can model it!\n",
        "# Let's use logistic regression again.\n",
        "from sklearn.metrics import roc_curve, auc, f1_score, recall_score, precision_score\n",
        "#recall_average = recall_score(y_test, y_predict, average=\"binary\", pos_label=\"neg\")\n",
        "\n",
        "df1 =df_moby_ali.copy()\n",
        "df = df_moby_per.copy()\n",
        "\n",
        "Y = df['text_source']\n",
        "X = np.array(df.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "Y1 = df1['text_source']\n",
        "X1 = np.array(df1.drop(['text_sentence','text_source'], 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.4, random_state=0)\n",
        "\n",
        "print(cross_val_score(lr,X_train, y_train, cv=kfolds, verbose = 2))\n",
        "pred_y_sklearn =cross_val_predict(lr, X_test, y_test, cv=10)\n",
        "y_true = y_test\n",
        "y_pred = cross_val_predict(lr, X_test, y_test, cv=10)\n",
        "print('\\n Austin Confusion Matrix')\n",
        "#print(\\n confusion_matrix(y_pred, y_true))\n",
        "print(pd.crosstab(pred_y_sklearn, y_test))\n",
        "print(\"\")\n",
        "print(classification_report(y_pred, y_true))\n",
        "\n",
        "print(cross_val_score(lr,X_train1, y_train1, cv=kfolds, verbose = 2))\n",
        "pred_y_sklearn1 =cross_val_predict(lr, X_test1, y_test1, cv=10)\n",
        "y_true1 = y_test1\n",
        "y_pred1 = cross_val_predict(lr, X_test1, y_test1, cv=10)\n",
        "print('\\n Carroll Confusion Matrix')\n",
        "#print(\\n confusion_matrix(y_pred, y_true))\n",
        "print(pd.crosstab(pred_y_sklearn1, y_test1))\n",
        "print(\"\")\n",
        "print(classification_report(y_pred1, y_true1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.4s\n",
            "[0.66666667 0.66666667 0.88888889 0.74074074 0.74074074 0.74074074\n",
            " 0.7037037  0.7037037  0.62962963 0.85185185]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Austin Confusion Matrix\n",
            "text_source  Austen  melville\n",
            "row_0                        \n",
            "Austen          101        15\n",
            "melville         24        38\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Austen       0.81      0.87      0.84       116\n",
            "    melville       0.72      0.61      0.66        62\n",
            "\n",
            "    accuracy                           0.78       178\n",
            "   macro avg       0.76      0.74      0.75       178\n",
            "weighted avg       0.78      0.78      0.78       178\n",
            "\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.2s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.3s\n",
            "[CV]  ................................................................\n",
            "[CV] ................................................. , total=   0.2s\n",
            "[0.75   0.8125 0.6875 0.75   0.75   0.9375 0.8125 0.75   0.8125 0.6875]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Carroll Confusion Matrix\n",
            "text_source  Carroll  melville\n",
            "row_0                         \n",
            "Carroll           42         8\n",
            "melville          11        43\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Carroll       0.79      0.84      0.82        50\n",
            "    melville       0.84      0.80      0.82        54\n",
            "\n",
            "    accuracy                           0.82       104\n",
            "   macro avg       0.82      0.82      0.82       104\n",
            "weighted avg       0.82      0.82      0.82       104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9W2QUOw_vvg",
        "colab_type": "text"
      },
      "source": [
        "#### When it comes to comparing Austin and Melville we get an F1 score for Austin of 84% and for Melville of only 66% I would trust our model less based on this result when it comes to other authors not used in the training.  I would probably suggest diversitfying the training sample to include more varieties of authors so we can create a more robust unique token list or use more advanced models or incorporate prefix, suffix, sentiment and other features to determine if they would add to the accuracy.  When we look at  Carroll compared to Melville the F1 score for both authors is 82%.  Based on these results I would say our model is effective at making the prediction a majority of the time but again I would want to improve the accuracy to over 95% using some more advanced optimization techiniques. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5N_pxoGz2VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}